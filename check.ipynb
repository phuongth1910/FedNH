{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.flbase.model import Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s01_PhuongModelNH(Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.return_embedding = config['FedNH_return_embedding']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,32,kernel_size=4,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32,64,kernel_size=4,stride=1,padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64,128,kernel_size=4,stride=1,padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128,128,kernel_size=4,stride=1,padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.pool2= nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(6*6*128,512)\n",
    "        self.fc2 = nn.Linear(512,config['num_classes'])\n",
    "        \n",
    "        temp = nn.Linear(512, config['num_classes'], bias=False).state_dict()['weight']\n",
    "        self.prototype = nn.Parameter(temp)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU() \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.scaling = torch.nn.Parameter(torch.tensor([1.0]))   \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        feature_embedding = x\n",
    "        feature_embedding_norm = torch.norm(feature_embedding, p=2, dim=1, keepdim=True).clamp(min=1e-12)\n",
    "        feature_embedding = torch.div(feature_embedding, feature_embedding_norm)\n",
    "\n",
    "        if not self.prototype.requires_grad:\n",
    "            normalized_prototype = self.prototype\n",
    "        else:\n",
    "            prototype_norm = torch.norm(self.prototype, p=2, dim=1, keepdim=True).clamp(min=1e-12)\n",
    "            normalized_prototype = torch.div(self.prototype, prototype_norm)\n",
    "\n",
    "        logits = torch.matmul(feature_embedding, normalized_prototype.T)\n",
    "        logits = self.scaling * logits\n",
    "\n",
    "        if self.return_embedding:\n",
    "            return feature_embedding, logits\n",
    "        else:\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s02_PhuongModelNH(Model):\n",
    "     def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.return_embedding = config['FedNH_return_embedding']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # 128 channels, 4x4 feature map after conv4\n",
    "        self.fc2 = nn.Linear(512, config['num_classes'])\n",
    "        \n",
    "        temp = nn.Linear(512, config['num_classes'], bias=False).state_dict()['weight']\n",
    "        self.prototype = nn.Parameter(temp)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU() \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.scaling = torch.nn.Parameter(torch.tensor([1.0]))\n",
    "        \n",
    "     def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        feature_embedding = x\n",
    "        feature_embedding_norm = torch.norm(feature_embedding, p=2, dim=1, keepdim=True).clamp(min=1e-12)\n",
    "        feature_embedding = torch.div(feature_embedding, feature_embedding_norm)\n",
    "\n",
    "        if not self.prototype.requires_grad:\n",
    "            normalized_prototype = self.prototype\n",
    "        else:\n",
    "            prototype_norm = torch.norm(self.prototype, p=2, dim=1, keepdim=True).clamp(min=1e-12)\n",
    "            normalized_prototype = torch.div(self.prototype, prototype_norm)\n",
    "\n",
    "        logits = torch.matmul(feature_embedding, normalized_prototype.T)\n",
    "        logits = self.scaling * logits\n",
    "\n",
    "        if self.return_embedding:\n",
    "            return feature_embedding, logits\n",
    "        else:\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhuongModelNH(Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.return_embedding = config['FedNH_return_embedding']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Update kích thước của lớp fully connected fc1\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # 128 channels, 4x4 feature map after conv4\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, config['num_classes'])\n",
    "        \n",
    "        temp = nn.Linear(512, config['num_classes'], bias=False).state_dict()['weight']\n",
    "        self.prototype = nn.Parameter(temp)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU() \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.scaling = torch.nn.Parameter(torch.tensor([1.0]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        print('After conv1:',x.size())\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        print('After conv2:',x.size())\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        print('After conv3:',x.size())\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        print('After conv4:',x.size())\n",
    "        x = self.flatten(x)\n",
    "        print('After flatten:',x.size())\n",
    "        x = self.relu(self.fc1(x))\n",
    "        print('After fc1:',x.size())\n",
    "        x = self.dropout(x)\n",
    "        print('After dropout:',x.size())\n",
    "        x = self.fc2(x)\n",
    "        print('After fc2:',x.size())\n",
    "        \n",
    "        feature_embedding = x\n",
    "        feature_embedding_norm = torch.norm(feature_embedding, p=2, dim=1, keepdim=True).clamp(min=1e-12)\n",
    "        feature_embedding = torch.div(feature_embedding, feature_embedding_norm)\n",
    "\n",
    "        if not self.prototype.requires_grad:\n",
    "            normalized_prototype = self.prototype\n",
    "        else:\n",
    "            prototype_norm = torch.norm(self.prototype, p=2, dim=1, keepdim=True).clamp(min=1e-12)\n",
    "            normalized_prototype = torch.div(self.prototype, prototype_norm)\n",
    "\n",
    "        logits = torch.matmul(feature_embedding, normalized_prototype.T)\n",
    "        logits = self.scaling * logits\n",
    "\n",
    "        if self.return_embedding:\n",
    "            return feature_embedding, logits\n",
    "        else:\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1: torch.Size([1, 32, 112, 112])\n",
      "After conv2: torch.Size([1, 64, 56, 56])\n",
      "After conv3: torch.Size([1, 128, 28, 28])\n",
      "After conv4: torch.Size([1, 128, 14, 14])\n",
      "After flatten: torch.Size([1, 25088])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x25088 and 2048x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Đưa ảnh qua mô hình\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 30\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFedNH_return_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     33\u001b[0m     feature_embedding, logits \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 43\u001b[0m, in \u001b[0;36mPhuongModelNH.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAfter flatten:\u001b[39m\u001b[38;5;124m'\u001b[39m,x\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m---> 43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAfter fc1:\u001b[39m\u001b[38;5;124m'\u001b[39m,x\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x25088 and 2048x512)"
     ]
    }
   ],
   "source": [
    "#transform\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "\n",
    "image_size = (224,224)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Đọc ảnh từ file\n",
    "image_path = 'D:/Comvis 2024/FedNH/data/tumorMRI/Training/glioma/augmented_image_0_Tr-gl_0032.jpg'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = transform(image)\n",
    "\n",
    "# Thêm batch dimension\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "# Định nghĩa cấu hình\n",
    "config = {\n",
    "    'FedNH_return_embedding': True,\n",
    "    'num_classes': 1\n",
    "}\n",
    "model = PhuongModelNH(config)\n",
    "model.eval()\n",
    "\n",
    "# Đưa ảnh qua mô hình\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "\n",
    "if config['FedNH_return_embedding']:\n",
    "    feature_embedding, logits = output\n",
    "    print('Feature Embedding:', feature_embedding)\n",
    "    print('Logits:', logits)\n",
    "else:\n",
    "    logits = output\n",
    "    print('Logits:', logits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
